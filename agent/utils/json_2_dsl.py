import json
from typing import Dict, List, Any, Optional
from collections import defaultdict


class OpenSearchStatsTranslator:
    """OpenSearchç»Ÿè®¡åˆ†æJSONç¿»è¯‘å™¨"""

    # æ”¯æŒçš„ç»Ÿè®¡æŒ‡æ ‡æ˜ å°„
    STATS_METRICS_MAP = {
        'count': 'value_count',
        'min': 'min',
        'max': 'max',
        'avg': 'avg',
        'sum': 'sum',
        'median': 'percentiles',
        'q1': 'percentiles',
        'q3': 'percentiles',
        'q5': 'percentiles',
        'std_deviation': 'extended_stats',
        'variance': 'extended_stats',
        'mode': 'terms',
        'cardinality': 'cardinality'
    }

    def __init__(self):
        self.query = {}

    def translate(self, input_json: Dict) -> Dict:
        """ä¸»ç¿»è¯‘æ–¹æ³•ï¼šJSONé…ç½®è½¬OpenSearch DSL"""
        try:
            query_type = input_json['query']['type']
            config = input_json['query']['config']

            self._build_base_query(config.get('filters', []))

            if query_type == 'stats':
                return self._build_stats_query(config)
            elif query_type == 'distribution':
                return self._build_distribution_query(config)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æŸ¥è¯¢ç±»å‹: {query_type}")

        except Exception as e:
            return {'error': str(e)}

    def _build_base_query(self, filters: List[Dict]) -> None:
        """æ„å»ºåŸºç¡€æŸ¥è¯¢æ¡ä»¶"""
        if not filters:
            self.query = {'match_all': {}}
            return

        bool_query = {'bool': {'must': []}}

        for filter_cond in filters:
            field = filter_cond['field']
            operator = filter_cond['operator']
            value = filter_cond.get('value')

            condition = self._build_filter_condition(field, operator, value)
            if condition:
                bool_query['bool']['must'].append(condition)

        self.query = bool_query

    def _build_filter_condition(self, field: str, operator: str, value: Any) -> Optional[Dict]:
        """æ„å»ºå•ä¸ªè¿‡æ»¤æ¡ä»¶"""
        if operator == 'eq':
            return {'term': {field: value}}
        elif operator == 'neq':
            return {'bool': {'must_not': [{'term': {field: value}}]}}
        elif operator == 'gt':
            return {'range': {field: {'gt': value}}}
        elif operator == 'gte':
            return {'range': {field: {'gte': value}}}
        elif operator == 'lt':
            return {'range': {field: {'lt': value}}}
        elif operator == 'lte':
            return {'range': {field: {'lte': value}}}
        elif operator == 'in':
            return {'terms': {field: value}}
        elif operator == 'range':
            return {'range': {field: value}}
        elif operator == 'exists':
            return {'exists': {'field': field}}
        elif operator == 'missing':
            return {'bool': {'must_not': [{'exists': {'field': field}}]}}
        else:
            return None

    def _build_stats_query(self, config: Dict) -> Dict:
        """æ„å»ºç»Ÿè®¡è®¡ç®—æŸ¥è¯¢"""
        fields = config.get('fields', [])
        metrics = config.get('metrics', ['min', 'max', 'avg', 'count', 'q1', 'median', 'q3'])

        aggs = {}
        for field in fields:
            field_aggs = {}
            for metric in metrics:
                if metric in ['median', 'q1', 'q3', 'q5']:
                    if 'percentiles' not in field_aggs:
                        field_aggs['percentiles'] = {
                            'percentiles': {'field': field, 'percents': []}
                        }
                    percent_value = 50 if metric == 'median' else 25 if metric == 'q1' else 75 if metric == 'q3' else 5
                    if percent_value not in field_aggs['percentiles']['percentiles']['percents']:
                        field_aggs['percentiles']['percentiles']['percents'].append(percent_value)

                elif metric in ['std_deviation', 'variance']:
                    if 'extended_stats' not in field_aggs:
                        field_aggs['extended_stats'] = {'extended_stats': {'field': field}}

                elif metric == 'mode':
                    field_aggs['mode'] = {
                        'terms': {'field': field, 'size': 1}
                    }

                else:
                    es_metric = self.STATS_METRICS_MAP.get(metric, metric)
                    field_aggs[metric] = {es_metric: {'field': field}}

            aggs[field] = {'aggs': field_aggs}

        return {
            'size': 0,
            'query': self.query,
            'aggs': aggs
        }

    def _build_distribution_query(self, config: Dict) -> Dict:
        """æ„å»ºåˆ†å¸ƒåˆ†ææŸ¥è¯¢ ï¼Œæ”¯æŒç™¾åˆ†æ¯”è®¡ç®—"""
        dimensions = config.get('dimensions', [])
        groups = config.get('groups', [])
        buckets = config.get('buckets', [])
        metrics = config.get('metrics', ['count', 'percentage'])
        metrics_field = config.get('metrics_field')

        # æ„å»ºèšåˆç»“æ„
        aggs = self._build_distribution_aggregations(
            dimensions, groups, buckets, metrics, metrics_field
        )

        return {
            'size': 0,
            'query': self.query,
            'aggs': aggs
        }

    def _build_distribution_aggregations(self, dimensions: List[str], groups: List[str],
                                         buckets: List[Dict], metrics: List[str],
                                         metrics_field: str) -> Dict:
        """æ„å»ºåˆ†å¸ƒåˆ†æçš„èšåˆç»“æ„"""
        aggs = {}
        current_level = aggs

        # æ·»åŠ æ€»è®¡æ•°ç”¨äºç™¾åˆ†æ¯”è®¡ç®—
        if 'percentage' in metrics:
            current_level['_total_count'] = {'value_count': {'field': '_index'}}

        # æ„å»ºåˆ†ç»„å±‚çº§
        for group_field in groups:
            current_level[group_field] = {
                'terms': {'field': group_field, 'size': 100},
                'aggs': {
                    '_group_count': {'value_count': {'field': '_index'}}  # åˆ†ç»„çº§åˆ«è®¡æ•°
                }
            }
            current_level = current_level[group_field]['aggs']

        # æ„å»ºæ¡¶èšåˆ
        for bucket in buckets:
            bucket_type = bucket['type']
            bucket_field = bucket['field']

            if bucket_type == 'terms':
                current_level[bucket_field] = {
                    'terms': {'field': bucket_field, 'size': bucket.get('size', 10)},
                    'aggs': {
                        '_bucket_count': {'value_count': {'field': '_index'}}  # æ¡¶çº§åˆ«è®¡æ•°
                    }
                }
                current_level = current_level[bucket_field]['aggs']

            elif bucket_type == 'range':
                range_ranges = []
                for range_def in bucket['ranges']:
                    range_spec = {}
                    if 'from' in range_def:
                        range_spec['from'] = range_def['from']
                    if 'to' in range_def:
                        range_spec['to'] = range_def['to']
                    if 'key' in range_def:
                        range_spec['key'] = range_def['key']
                    range_ranges.append(range_spec)

                current_level[bucket_field] = {
                    'range': {'field': bucket_field, 'ranges': range_ranges},
                    'aggs': {
                        '_bucket_count': {'value_count': {'field': '_index'}}
                    }
                }
                current_level = current_level[bucket_field]['aggs']

            elif bucket_type == 'date_histogram':
                current_level[bucket_field] = {
                    'date_histogram': {
                        'field': bucket_field,
                        'interval': bucket['interval'],
                        'format': bucket.get('format', 'yyyy-MM')
                    },
                    'aggs': {
                        '_bucket_count': {'value_count': {'field': '_index'}}
                    }
                }
                current_level = current_level[bucket_field]['aggs']

        # æ„å»ºç»´åº¦èšåˆ
        for dim_field in dimensions:
            current_level[dim_field] = {
                'terms': {'field': dim_field, 'size': 100},
                'aggs': {
                    '_dimension_count': {'value_count': {'field': '_index'}}
                }
            }
            current_level = current_level[dim_field]['aggs']

        # æ·»åŠ æŒ‡æ ‡è®¡ç®—
        self._add_metrics_aggregations(current_level, metrics, metrics_field)

        return aggs

    def _add_metrics_aggregations(self, aggs: Dict, metrics: List[str], metrics_field: str) -> None:
        """æ·»åŠ æŒ‡æ ‡è®¡ç®—èšåˆ"""
        for metric in metrics:
            if metric == 'count':
                aggs['count'] = {'value_count': {'field': '_index'}}
            elif metric in ['avg', 'sum', 'min', 'max'] and metrics_field:
                es_metric = self.STATS_METRICS_MAP.get(metric, metric)
                aggs[metric] = {es_metric: {'field': metrics_field}}

    def process_stats_result(self, es_result: Dict, original_config: Dict) -> Dict:
        """å¤„ç†ç»Ÿè®¡è®¡ç®—ç»“æœ"""
        try:
            result = {}
            config = original_config['query']['config']
            fields = config.get('fields', [])
            metrics = config.get('metrics', ['min', 'max', 'avg', 'count', 'q1', 'median', 'q3'])

            aggregations = es_result.get('aggregations', {})

            for field in fields:
                field_result = {}
                field_aggs = aggregations.get(field, {})

                for metric in metrics:
                    if metric == 'count':
                        field_result['count'] = field_aggs.get('count', {}).get('value', 0)
                    elif metric in ['min', 'max', 'avg', 'sum']:
                        field_result[metric] = field_aggs.get(metric, {}).get('value')
                    elif metric in ['std_deviation', 'variance']:
                        ext_stats = field_aggs.get('extended_stats', {})
                        if metric == 'std_deviation':
                            field_result['std_deviation'] = ext_stats.get('std_deviation')
                        else:
                            field_result['variance'] = ext_stats.get('variance')
                    elif metric in ['median', 'q1', 'q3', 'q5']:
                        percentiles = field_aggs.get('percentiles', {}).get('values', {})
                        key = '50.0' if metric == 'median' else '25.0' if metric == 'q1' else '75.0' if metric == 'q3' else '5.0'
                        field_result[metric] = percentiles.get(key)
                    elif metric == 'mode':
                        buckets = field_aggs.get('mode', {}).get('buckets', [])
                        if buckets:
                            field_result['mode'] = buckets[0].get('key')
                            field_result['mode_count'] = buckets[0].get('doc_count')

                result[field] = field_result

            return result

        except Exception as e:
            return {'error': f'ç»“æœå¤„ç†é”™è¯¯: {str(e)}'}

    def process_distribution_result(self, es_result: Dict, original_config: Dict) -> Dict:
        """å¤„ç†åˆ†å¸ƒåˆ†æç»“æœï¼Œæ”¯æŒç™¾åˆ†æ¯”è®¡ç®—"""
        try:
            aggregations = es_result.get('aggregations', {})
            config = original_config['query']['config']

            # è·å–æ€»è®¡æ•°ç”¨äºç™¾åˆ†æ¯”è®¡ç®—
            total_count = aggregations.get('_total_count', {}).get('value', 0)

            return self._process_distribution_aggregations(
                aggregations, config, total_count, level=0
            )

        except Exception as e:
            return {'error': f'åˆ†å¸ƒç»“æœå¤„ç†é”™è¯¯: {str(e)}'}

    def _process_distribution_aggregations(self, aggs: Dict, config: Dict,
                                           parent_total: int, level: int = 0) -> Dict:
        """é€’å½’å¤„ç†åˆ†å¸ƒåˆ†æèšåˆç»“æœ"""
        result = {'buckets': []}

        # è·å–å½“å‰å±‚çº§çš„èšåˆé”®
        aggregation_keys = [k for k in aggs.keys() if not k.startswith('_')]

        for agg_key in aggregation_keys:
            agg_data = aggs[agg_key]

            if 'buckets' in agg_data:
                # å¤„ç†æ¡¶èšåˆ
                buckets = agg_data['buckets']
                current_level_total = sum(bucket.get('doc_count', 0) for bucket in buckets)

                for bucket in buckets:
                    bucket_result = self._process_bucket(bucket, config, parent_total, current_level_total, level)

                    # é€’å½’å¤„ç†å­èšåˆ
                    sub_aggs = {k: v for k, v in bucket.items()
                                if k not in ['key', 'from', 'to', 'doc_count', 'key_as_string']}

                    if sub_aggs and level < 5:  # é˜²æ­¢æ— é™é€’å½’
                        sub_result = self._process_distribution_aggregations(
                            sub_aggs, config, bucket.get('doc_count', 0), level + 1
                        )
                        if sub_result.get('buckets'):
                            bucket_result['sub_aggregations'] = sub_result

                    result['buckets'].append(bucket_result)

        return result

    def _process_bucket(self, bucket: Dict, config: Dict,
                        parent_total: int, current_level_total: int, level: int) -> Dict:
        """å¤„ç†å•ä¸ªæ¡¶çš„ç»“æœ"""
        bucket_result = {
            'key': bucket.get('key'),
            'key_as_string': bucket.get('key_as_string'),
            'from': bucket.get('from'),
            'to': bucket.get('to'),
            'doc_count': bucket.get('doc_count', 0)
        }

        metrics = config.get('metrics', ['count', 'percentage'])
        metrics_field = config.get('metrics_field')

        # è®¡ç®—æŒ‡æ ‡
        metrics_result = {}

        for metric in metrics:
            if metric == 'count':
                metrics_result['count'] = bucket.get('doc_count', 0)

            elif metric == 'percentage':
                # è®¡ç®—ç™¾åˆ†æ¯”ï¼šå½“å‰æ¡¶è®¡æ•° / çˆ¶çº§æ€»è®¡æ•° * 100
                if parent_total > 0:
                    percentage = (bucket.get('doc_count', 0) / parent_total) * 100
                    metrics_result['percentage'] = round(percentage, 2)
                else:
                    metrics_result['percentage'] = 0.0

            elif metric in ['avg', 'sum', 'min', 'max'] and metrics_field:
                metric_value = bucket.get(metric, {}).get('value')
                if metric_value is not None:
                    metrics_result[metric] = metric_value

        if metrics_result:
            bucket_result['metrics'] = metrics_result

        return bucket_result


def demo_enhanced_distribution():
    """æ¼”ç¤ºåˆ†å¸ƒåˆ†æåŠŸèƒ½"""
    translator = OpenSearchStatsTranslator()

    # æµ‹è¯•ç”¨ä¾‹ï¼šä¸åŒéƒ¨é—¨ã€å¹´é¾„æ®µçš„è–ªèµ„åˆ†å¸ƒ
    test_query = {
        "query": {
            "type": "distribution",
            "config": {
                "dimensions": ["education"],
                "groups": ["department"],
                "buckets": [
                    {
                        "type": "range",
                        "field": "age",
                        "ranges": [
                            {"key": "20-30", "from": 20, "to": 30},
                            {"key": "30-40", "from": 30, "to": 40}
                        ]
                    }
                ],
                "metrics": ["count", "percentage", "avg"],
                "metrics_field": "salary",
                "filters": [
                    {
                        "field": "active",
                        "operator": "eq",
                        "value": True
                    }
                ]
            }
        }
    }

    # ç”ŸæˆDSL
    dsl = translator.translate(test_query)
    print("1. ç”Ÿæˆçš„DSLï¼ˆåŒ…å«ç™¾åˆ†æ¯”è®¡ç®—ï¼‰:")
    print(json.dumps(dsl, indent=2, ensure_ascii=False))

    # æ¨¡æ‹ŸOpenSearchè¿”å›ç»“æœ
    mock_result = {
        "aggregations": {
            "_total_count": {"value": 1000},
            "department": {
                "buckets": [
                    {
                        "key": "engineering",
                        "doc_count": 600,
                        "_group_count": {"value": 600},
                        "age": {
                            "buckets": [
                                {
                                    "key": "20-30",
                                    "from": 20,
                                    "to": 30,
                                    "doc_count": 300,
                                    "_bucket_count": {"value": 300},
                                    "education": {
                                        "buckets": [
                                            {
                                                "key": "bachelor",
                                                "doc_count": 200,
                                                "_dimension_count": {"value": 200},
                                                "count": {"value": 200},
                                                "avg": {"value": 15000}
                                            },
                                            {
                                                "key": "master",
                                                "doc_count": 100,
                                                "_dimension_count": {"value": 100},
                                                "count": {"value": 100},
                                                "avg": {"value": 20000}
                                            }
                                        ]
                                    }
                                },
                                {
                                    "key": "30-40",
                                    "from": 30,
                                    "to": 40,
                                    "doc_count": 300,
                                    "_bucket_count": {"value": 300},
                                    "education": {
                                        "buckets": [
                                            {
                                                "key": "bachelor",
                                                "doc_count": 180,
                                                "_dimension_count": {"value": 180},
                                                "count": {"value": 180},
                                                "avg": {"value": 25000}
                                            },
                                            {
                                                "key": "master",
                                                "doc_count": 120,
                                                "_dimension_count": {"value": 120},
                                                "count": {"value": 120},
                                                "avg": {"value": 30000}
                                            }
                                        ]
                                    }
                                }
                            ]
                        }
                    },
                    {
                        "key": "sales",
                        "doc_count": 400,
                        "_group_count": {"value": 400},
                        "age": {
                            "buckets": [
                                {
                                    "key": "20-30",
                                    "from": 20,
                                    "to": 30,
                                    "doc_count": 200,
                                    "_bucket_count": {"value": 200},
                                    "education": {
                                        "buckets": [
                                            {
                                                "key": "bachelor",
                                                "doc_count": 150,
                                                "_dimension_count": {"value": 150},
                                                "count": {"value": 150},
                                                "avg": {"value": 12000}
                                            }
                                        ]
                                    }
                                }
                            ]
                        }
                    }
                ]
            }
        }
    }

    print("\n2. æ¨¡æ‹Ÿçš„OpenSearchè¿”å›ç»“æœ:")
    print(json.dumps(mock_result, indent=2, ensure_ascii=False))

    # å¤„ç†ç»“æœ
    processed_result = translator.process_distribution_result(mock_result, test_query)
    print("\n3. å¤„ç†åçš„åˆ†å¸ƒåˆ†æç»“æœï¼ˆåŒ…å«ç™¾åˆ†æ¯”ï¼‰:")
    print(json.dumps(processed_result, indent=2, ensure_ascii=False))


def test_basic_stats_query():
    """æµ‹è¯•åŸºç¡€ç»Ÿè®¡æŸ¥è¯¢åŠŸèƒ½"""
    print("=== æµ‹è¯•1: åŸºç¡€ç»Ÿè®¡æŸ¥è¯¢ ===")

    translator = OpenSearchStatsTranslator()

    # åŸºç¡€ç»Ÿè®¡æŸ¥è¯¢
    basic_stats = {
        "query": {
            "type": "stats",
            "config": {
                "fields": ["price", "quantity"],
                "metrics": ["min", "max", "avg", "count"]
            }
        }
    }

    dsl = translator.translate(basic_stats)
    print("ç”Ÿæˆçš„DSL:")
    print(json.dumps(dsl, indent=2, ensure_ascii=False))

    # æ¨¡æ‹Ÿè¿”å›ç»“æœ
    mock_result = {
        "aggregations": {
            "price": {
                "min": {"value": 10},
                "max": {"value": 100},
                "avg": {"value": 55.5},
                "count": {"value": 50}
            },
            "quantity": {
                "min": {"value": 1},
                "max": {"value": 20},
                "avg": {"value": 8.5},
                "count": {"value": 50}
            }
        }
    }

    result = translator.process_stats_result(mock_result, basic_stats)
    print("\nå¤„ç†åçš„ç»Ÿè®¡ç»“æœ:")
    print(json.dumps(result, indent=2, ensure_ascii=False))

    # éªŒè¯ç»“æœ
    assert "price" in result
    assert result["price"]["min"] == 10
    assert result["price"]["max"] == 100
    assert result["price"]["avg"] == 55.5
    print("âœ“ åŸºç¡€ç»Ÿè®¡æŸ¥è¯¢æµ‹è¯•é€šè¿‡")


def test_stats_with_filters():
    """æµ‹è¯•å¸¦è¿‡æ»¤æ¡ä»¶çš„ç»Ÿè®¡æŸ¥è¯¢"""
    print("\n=== æµ‹è¯•2: å¸¦è¿‡æ»¤çš„ç»Ÿè®¡æŸ¥è¯¢ ===")

    translator = OpenSearchStatsTranslator()

    stats_with_filters = {
        "query": {
            "type": "stats",
            "config": {
                "fields": ["salary"],
                "metrics": ["min", "max", "avg", "median", "std_deviation"],
                "filters": [
                    {
                        "field": "department",
                        "operator": "eq",
                        "value": "engineering"
                    },
                    {
                        "field": "age",
                        "operator": "gte",
                        "value": 25
                    },
                    {
                        "field": "salary",
                        "operator": "lt",
                        "value": 100000
                    }
                ]
            }
        }
    }

    dsl = translator.translate(stats_with_filters)
    print("ç”Ÿæˆçš„DSL:")
    print(json.dumps(dsl, indent=2, ensure_ascii=False))

    # éªŒè¯è¿‡æ»¤æ¡ä»¶æ˜¯å¦æ­£ç¡®è½¬æ¢
    query = dsl.get("query", {})
    assert "bool" in query
    assert "must" in query["bool"]
    assert len(query["bool"]["must"]) == 3
    print("âœ“ è¿‡æ»¤æ¡ä»¶è½¬æ¢æ­£ç¡®")


def test_basic_distribution():
    """æµ‹è¯•åŸºç¡€åˆ†å¸ƒåˆ†æ"""
    print("\n=== æµ‹è¯•3: åŸºç¡€åˆ†å¸ƒåˆ†æ ===")

    translator = OpenSearchStatsTranslator()

    basic_dist = {
        "query": {
            "type": "distribution",
            "config": {
                "dimensions": ["category"],
                "metrics": ["count", "percentage"]
            }
        }
    }

    dsl = translator.translate(basic_dist)
    print("ç”Ÿæˆçš„DSL:")
    print(json.dumps(dsl, indent=2, ensure_ascii=False))

    # æ¨¡æ‹Ÿç®€å•åˆ†å¸ƒç»“æœ
    mock_result = {
        "aggregations": {
            "_total_count": {"value": 1000},
            "category": {
                "buckets": [
                    {
                        "key": "electronics",
                        "doc_count": 400,
                        "_dimension_count": {"value": 400},
                        "count": {"value": 400}
                    },
                    {
                        "key": "books",
                        "doc_count": 350,
                        "_dimension_count": {"value": 350},
                        "count": {"value": 350}
                    },
                    {
                        "key": "clothing",
                        "doc_count": 250,
                        "_dimension_count": {"value": 250},
                        "count": {"value": 250}
                    }
                ]
            }
        }
    }

    result = translator.process_distribution_result(mock_result, basic_dist)
    print("\nå¤„ç†åçš„åˆ†å¸ƒç»“æœ:")
    print(json.dumps(result, indent=2, ensure_ascii=False))

    # éªŒè¯ç™¾åˆ†æ¯”è®¡ç®—
    buckets = result.get("buckets", [])
    for bucket in buckets:
        metrics = bucket.get("metrics", {})
        if metrics.get("count") == 400:
            assert metrics.get("percentage") == 40.0  # 400/1000 * 100
        elif metrics.get("count") == 350:
            assert metrics.get("percentage") == 35.0
    print("âœ“ åŸºç¡€åˆ†å¸ƒåˆ†ææµ‹è¯•é€šè¿‡")


def test_terms_bucket_distribution():
    """æµ‹è¯•æœ¯è¯­æ¡¶åˆ†å¸ƒåˆ†æ"""
    print("\n=== æµ‹è¯•4: æœ¯è¯­æ¡¶åˆ†å¸ƒåˆ†æ ===")

    translator = OpenSearchStatsTranslator()

    terms_dist = {
        "query": {
            "type": "distribution",
            "config": {
                "dimensions": ["sub_category"],
                "buckets": [
                    {
                        "type": "terms",
                        "field": "main_category",
                        "size": 5
                    }
                ],
                "metrics": ["count", "percentage", "avg"],
                "metrics_field": "price"
            }
        }
    }

    dsl = translator.translate(terms_dist)
    print("ç”Ÿæˆçš„DSL:")
    print(json.dumps(dsl, indent=2, ensure_ascii=False))

    # éªŒè¯æœ¯è¯­æ¡¶é…ç½®
    aggs = dsl.get("aggs", {})
    assert "main_category" in aggs
    assert aggs["main_category"]["terms"]["size"] == 5
    print("âœ“ æœ¯è¯­æ¡¶é…ç½®æ­£ç¡®")


def test_range_bucket_distribution():
    """æµ‹è¯•èŒƒå›´æ¡¶åˆ†å¸ƒåˆ†æ"""
    print("\n=== æµ‹è¯•5: èŒƒå›´æ¡¶åˆ†å¸ƒåˆ†æ ===")

    translator = OpenSearchStatsTranslator()

    range_dist = {
        "query": {
            "type": "distribution",
            "config": {
                "dimensions": ["status"],
                "buckets": [
                    {
                        "type": "range",
                        "field": "price",
                        "ranges": [
                            {"key": "ä½ä»·", "from": 0, "to": 100},
                            {"key": "ä¸­ä»·", "from": 100, "to": 500},
                            {"key": "é«˜ä»·", "from": 500}
                        ]
                    }
                ],
                "metrics": ["count", "percentage"]
            }
        }
    }

    dsl = translator.translate(range_dist)
    print("ç”Ÿæˆçš„DSL:")
    print(json.dumps(dsl, indent=2, ensure_ascii=False))

    # éªŒè¯èŒƒå›´æ¡¶é…ç½®
    aggs = dsl.get("aggs", {})
    assert "price" in aggs
    assert len(aggs["price"]["range"]["ranges"]) == 3
    print("âœ“ èŒƒå›´æ¡¶é…ç½®æ­£ç¡®")


def test_complex_metrics():
    """æµ‹è¯•å¤æ‚æŒ‡æ ‡è®¡ç®—"""
    print("\n=== æµ‹è¯•6: å¤æ‚æŒ‡æ ‡è®¡ç®— ===")

    translator = OpenSearchStatsTranslator()

    complex_stats = {
        "query": {
            "type": "stats",
            "config": {
                "fields": ["score"],
                "metrics": ["min", "max", "avg", "median", "q1", "q3", "std_deviation", "variance", "mode"]
            }
        }
    }

    dsl = translator.translate(complex_stats)
    print("ç”Ÿæˆçš„DSL:")
    print(json.dumps(dsl, indent=2, ensure_ascii=False))

    # æ¨¡æ‹ŸåŒ…å«å¤æ‚æŒ‡æ ‡çš„ç»“æœ
    mock_result = {
        "aggregations": {
            "score": {
                "min": {"value": 0},
                "max": {"value": 100},
                "avg": {"value": 75.5},
                "percentiles": {
                    "values": {
                        "25.0": 60.0,
                        "50.0": 75.0,
                        "75.0": 90.0
                    }
                },
                "extended_stats": {
                    "std_deviation": 15.2,
                    "variance": 231.04
                },
                "mode": {
                    "buckets": [
                        {"key": 80, "doc_count": 25}
                    ]
                }
            }
        }
    }

    result = translator.process_stats_result(mock_result, complex_stats)
    print("\nå¤„ç†åçš„å¤æ‚æŒ‡æ ‡ç»“æœ:")
    print(json.dumps(result, indent=2, ensure_ascii=False))

    # éªŒè¯å¤æ‚æŒ‡æ ‡
    score_result = result.get("score", {})
    assert score_result["min"] == 0
    assert score_result["max"] == 100
    assert score_result["avg"] == 75.5
    assert score_result["q1"] == 60.0
    assert score_result["median"] == 75.0
    assert score_result["q3"] == 90.0
    assert score_result["std_deviation"] == 15.2
    assert score_result["variance"] == 231.04
    assert score_result["mode"] == 80
    print("âœ“ å¤æ‚æŒ‡æ ‡è®¡ç®—æµ‹è¯•é€šè¿‡")


def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    print("\n=== æµ‹è¯•7: é”™è¯¯å¤„ç† ===")

    translator = OpenSearchStatsTranslator()

    # æµ‹è¯•æ— æ•ˆæŸ¥è¯¢ç±»å‹
    invalid_query = {
        "query": {
            "type": "invalid_type",
            "config": {
                "fields": ["test"]
            }
        }
    }

    result = translator.translate(invalid_query)
    assert "error" in result
    print("âœ“ æ— æ•ˆæŸ¥è¯¢ç±»å‹é”™è¯¯å¤„ç†æ­£ç¡®")

    # æµ‹è¯•ç¼ºå°‘å¿…è¦å­—æ®µ
    missing_fields = {
        "query": {
            "type": "stats",
            "config": {
                # ç¼ºå°‘fieldså­—æ®µ
            }
        }
    }

    result = translator.translate(missing_fields)
    # åº”è¯¥èƒ½æ­£å¸¸å¤„ç†ç©ºå­—æ®µåˆ—è¡¨
    assert "aggs" in result
    print("âœ“ ç¼ºå¤±å­—æ®µå¤„ç†æ­£ç¡®")


def test_percentage_calculation_edge_cases():
    """æµ‹è¯•ç™¾åˆ†æ¯”è®¡ç®—çš„è¾¹ç•Œæƒ…å†µ"""
    print("\n=== æµ‹è¯•8: ç™¾åˆ†æ¯”è®¡ç®—è¾¹ç•Œæƒ…å†µ ===")

    translator = OpenSearchStatsTranslator()

    # æµ‹è¯•é™¤é›¶æƒ…å†µ
    mock_zero_result = {
        "aggregations": {
            "_total_count": {"value": 0},  # æ€»æ•°ä¸º0
            "category": {
                "buckets": [
                    {
                        "key": "test",
                        "doc_count": 0,
                        "_dimension_count": {"value": 0},
                        "count": {"value": 0}
                    }
                ]
            }
        }
    }

    test_query = {
        "query": {
            "type": "distribution",
            "config": {
                "dimensions": ["category"],
                "metrics": ["count", "percentage"]
            }
        }
    }

    result = translator.process_distribution_result(mock_zero_result, test_query)
    buckets = result.get("buckets", [])
    if buckets:
        metrics = buckets[0].get("metrics", {})
        assert metrics.get("percentage") == 0.0  # é™¤é›¶æ—¶åº”è¿”å›0

    print("âœ“ é™¤é›¶æƒ…å†µå¤„ç†æ­£ç¡®")


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("å¼€å§‹è¿è¡ŒOpenSearchç¿»è¯‘å™¨æµ‹è¯•...\n")

    try:
        test_basic_stats_query()
        test_stats_with_filters()
        test_basic_distribution()
        test_terms_bucket_distribution()
        test_range_bucket_distribution()
        test_complex_metrics()
        test_error_handling()
        test_percentage_calculation_edge_cases()

        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    demo_enhanced_distribution()

    print("\n" + "=" * 60)
    # è¿è¡Œæµ‹è¯•
    run_all_tests()


